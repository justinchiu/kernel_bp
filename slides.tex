\documentclass{beamer}
 
\usepackage[utf8]{inputenc}
\usepackage{mystyle}
\usepackage{diagbox}
\usepackage{subcaption}
\usepackage{tikz}

\usepgfmodule{shapes}
\usepgfmodule{plot}
\usetikzlibrary{decorations}
\usetikzlibrary{arrows}
\usetikzlibrary{shadows}  
\usetikzlibrary{snakes} 
\usetikzlibrary{shapes}
\usetikzlibrary{matrix} 
\usetikzlibrary{positioning} 
\usetikzlibrary{backgrounds}
 

\newcommand{\LSE}{\textrm{LSE}}

%Information to be included in the title page:
\title{Kernel Belief Propagation}

\begin{document}
 
\frame{\titlepage}
 
\begin{frame}
\frametitle{Motivation}
\begin{itemize}
\item Difficult to derive belief propagation messages for continuous RVs
    with complex densities, which typically rely on easy to compute conditionals
    (ie conjugacy or discrete)
\vspace{1em}
\item Instead, rewrite messages using \textbf{nonparametric} representations
of densities, i.e. sums of points in some space with no explicit parameters
\vspace{1em}
\item Approach extends to any domain on which kernels can be defined,
such as strings and graphs
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Why is this interesting?}
\begin{itemize}
\item Background is relevant for spectral + kernel methods in latent variable models 
\vspace{2em}
\item Keyword overlap with recent work in efficient attention
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Preview: Comparison with Performer-style Inference}
\begin{itemize}
\item Both do inference in embedded space using kernels
\vspace{1em}
\item 3 terms in complexity: size of state space, feature dimension, number of samples
\vspace{1em}
\item Performer relies on explicit computation of inner products with RFF,
    so complexity is a function of feature dimension and size of state space
\vspace{1em}
\item KBP emphasizes approximate nonparametric inference, complexity is a
    function of number of samples.
    Feature dimension is always avoided with kernel trick.
    Approximations are made to reduce dependence on number of samples.
\end{itemize}
\end{frame}


\begin{frame}
\frametitle{Learning in Markov Random Fields}
\begin{itemize}
\item Pairwise MRF (typically parameterize log potentials)
$$\mathbb{P}(X) \propto \prod_{s,t \in \mcE} \Psi_{st}(X_s, X_t) \prod_{s\in\mcV} \Psi(X_s).$$
\item Estimate gradients wrt log potentials by computing edge and node marginals
via inference, ie the beliefs $\mathbb{B}(X_s, X_t)$ and $\mathbb{B}(X_s)$
\vspace{2em}
\item Belief propagation is an algorithm for performing inference
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Belief Prop (BP)}
\begin{itemize}
\item BP propagates messages from nodes to neighbours iteratively until convergence
\vspace{1em}
\item Messages from node $t$ to $s$
$$m_{ts}(X_s) = \int_{X_t\in\mcX} \Psi_{st}(X_s, X_t) \Psi_t(X_t)
    \prod_{u\in\delta(t)\setminus\set{s}} m_{ut}(X_t)dX_t$$
\item Belief at $s$
$$\mathbb{B}(X_s) = \Psi_s(X_s)\prod_{t\in\delta(s)} m_{ts}^*(X_s),$$
with fixed point messages $m^*$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{BP}
\begin{itemize}
\item The integrals in the messages may be difficult to compute
\vspace{1em}
\item Solution: Rewrite messages as an expectation (by dividing by fixed point messages)\footnote{
Is this an existence statement about messages?
},
then approximate conditional
\begin{align*}
m_{ts}'(X_s) &= \int_\mcX \mathbb{P}^*(X_t \mid X_s)
\prod_{u \in \delta(t)\setminus\set{s}} m_{ut}'(X_t)dX_t\\
&= \mathbb{E}_{Xt \mid X_s}\left[
\prod_{u \in \delta(t)\setminus\set{s}} m_{ut}'(X_t)\right]
\end{align*}
\item Requires fully observed model, otherwise stuck with original integral
\vspace{1em}
\item *Take closer look at reparameterization (eqns 1-4),
    discuss how that affects algorithm
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Issues with Nonparametric BP Baselines}
\begin{itemize}
\item NPBP baselines are Gaussian Mixture BP (Sudderth et al, 2003) and 
Particle BP (Ihler and McAllester, 2009)
\vspace{2em}
\item They claim NPBP requires a 2-step process of estimating conditional $\mathbb{P}^*(X_t \mid X_s)$,
    then computing messages
\vspace{2em}
\item Kernel BP reduces this to a single step of matrix-vector products
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{High Level Overview of Kernel Belief Propagation}
\begin{itemize}
\item Embed messages in RKHS
\vspace{2em}
\item Approximate expectations via observed samples 
\vspace{2em}
\item Compute messages with inner products
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Warmup: Kernel Mean Embedding}
%% schematic summarization
\begin{figure}[t!]
  \centering
  \begin{tikzpicture}[domain=0:6,scale=0.7]
      % set up locations
      \coordinate (P1) at (3.4,1.6);
      \coordinate (Q1) at (3.8,2.4);
      \coordinate (P2) at (12,2);
      \coordinate (Q2) at (11,3);

      \draw[->] (-0.2,1) -- (6,1) node[right] {$\ensuremath{\bx}$};
      \draw[->] (0,1) -- (0,4.2) node[above] {$p(\bx)$};
      \draw[color=red,smooth,domain=0:5.5,ultra thick] plot (\x,{1+2*exp(-(\x-3)*(\x-3)*0.5/1)});
      \draw[color=blue,smooth,domain=0:5,ultra thick] plot (\x,{1+3*exp(-(\x-2)*(\x-2)*0.5/0.6)});

      % the rkhs
      \node [cloud, fill=green!50, cloud puffs=16, cloud puff arc= 100,
      minimum width=4cm, minimum height=2.5cm, aspect=1] at (12.5,2.5) {};
      \node [] at (12.5,4.6) {\textbf{RKHS} $\mathscr{H}$};

      % draw the embedding
      \draw[color=gray] (P1) to[out=20,in=170] (P2);
      \draw[color=gray] (Q1) to[out=20,in=170] (Q2);

      \node [fill=blue,circle,thick,minimum width=0.1cm] at (P2) {};
      \node [right] at (12.2,2) {$\mu_{\mathbb{P}}$};
      \node [fill=red,circle,thick,,minimum width=0.1cm] at (Q2) {};
      \node [right] at (11.2,3) {$\mu_{\mathbb{Q}}$};

      \node [right] at (2.5,4) {$\color{blue} \mathbb{P}$};
      \node [right] at (3.5,3) {$\color{red} \mathbb{Q}$};
  \end{tikzpicture}
  %\caption{Embedding of marginal distributions: Each distribution is mapped into a reproducing
   % kernel Hilbert space via an expectation operation.}
  \label{fig:kme-summary}
\end{figure}

\begin{itemize}
\item Kernel mean embeddings map distributions into Hilbert spaces
\item Can approximate embedding in RKHS via sampling
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel Mean Embedding}
\begin{itemize}
\item Definition: 
$$\mu_X(\cdot) = \mathbb{E}_X[\phi_X(\cdot)],$$
with feature map $\phi = k(x, \cdot) \in \mathscr{H}$
\vspace{1em}
\item Goal in this paper is to write everything as an inner product and then apply
kernel trick
\vspace{1em}
\item Using the reproducing property and linearity:
$$\mathbb{E}_X[f(X)]
= \mathbb{E}_X[\langle f, \phi_X \rangle]
= \langle f, \mathbb{E}_X[\phi_X] \rangle
= \langle f, \mu_X\rangle, \forall f \in \mathscr{H},$$
so we can write expected functions of RVs as inner products
\vspace{1em}
\item Consider categorical distribution with $\phi(\bx)$ one-hot:
    $\mu_X = P(X)$.
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel Mean Embedding: Empirical Estimation}
\begin{itemize}
\item Some notation: With samples $\set{X^n}_{n=1}^N\sim \mathbb{P}(X)$,
let $\Phi= [\phi(X^1), \cdots, \phi(X^N)] \in \R^{D\times N}$ be the feature matrices
(assuming finite dim $D$ feature space)
\vspace{2em}
\item We can approximate $\mu_X \approx \Phi^\top\mathbf{1} / m$
\vspace{2em}
\item Will use $\Phi$ again later, in the form of kernel matrix
    $K = \Phi^\top\Phi$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conditional Mean Embedding}
\begin{figure}[t!]
\centering
  \begin{tikzpicture}[scale=0.6]
      %\draw[->] (-0.2,0.5) -- (7.5,0.5) node[right] {$X$};
      %\draw[->] (0,0.5) -- (0,3.7) node[left,yshift=-10] {$Y$};
      %\draw[rotate=-90,color=blue!10,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{1+exp(-(3+\x)*(3+\x)*0.5/0.2)});
      %\draw[rotate=-90,color=blue!15,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{1.6+exp(-(2.8+\x)*(2.8+\x)*0.5/0.3)});
      %\draw[rotate=-90,color=blue!20,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{2.2+exp(-(2.6+\x)*(2.6+\x)*0.5/0.3)});
      %\draw[rotate=-90,color=blue!25,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{2.8+exp(-(2.4+\x)*(2.4+\x)*0.5/0.3)});
      %\draw[rotate=-90,color=blue!30,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{3.4+exp(-(2.2+\x)*(2.2+\x)*0.5/0.3)});
      %\draw[rotate=-90,color=blue!35,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{4+exp(-(2.4+\x)*(2.4+\x)*0.5/0.3)});
      %\draw[rotate=-90,color=blue!40,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{4.6+exp(-(2.5+\x)*(2.5+\x)*0.5/0.5)});
      %\draw[rotate=-90,color=blue!45,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{5.2+exp(-(2.6+\x)*(2.6+\x)*0.5/0.5)});
      %\draw[rotate=-90,color=blue!50,smooth,domain=-3.5:-0.5,ultra thick] plot (\x,{5.8+exp(-(2.9+\x)*(2.9+\x)*0.5/0.6)});
      %\node [rotate=-90] at (0.6,2.6) {$P(Y|X=x)$};
      %\node [rotate=-90] at (4,4.5) {$P(Y|X)$};
      %\node [rotate=-90] at (0.6,2.6) {$\mathbb{P}(Y|X)$};

      %%%%%%

      % set up locations
      \coordinate (P1) at (-2.3,5.7);
      \coordinate (P2) at (9,5.7);

      %\draw[->] (-0.2,1) -- (6,1) node[right] {$x$};
      %\draw[->] (0,1) -- (0,4.2) node[above] {$p(x)$};
      %\draw[color=red,smooth,domain=0:5.5,ultra thick] plot (\x,{1+2*exp(-(\x-3)*(\x-3)*0.5/1)});
      %\draw[color=blue,smooth,domain=0:5,ultra thick] plot (\x,{1+3*exp(-(\x-2)*(\x-2)*0.5/0.6)});

      %%%%%%

      % the rkhs of X
      \node [cloud, fill=green!50, cloud puffs=16, cloud puff arc= 100,
      minimum width=3.4cm, minimum height=2.2cm, aspect=1] at (-2.5,5) {};
      \node [] at (-2.5,7) {\textbf{RKHS} $\mathscr{H}$};

      % the rkhs of Y
      \node [cloud, fill=green!50, cloud puffs=16, cloud puff arc= 100,
      minimum width=3.4cm, minimum height=2.2cm, aspect=1] at (9.5,5) {};
      \node [] at (9.5,7) {\textbf{RKHS} $\mathscr{G}$};

      %%%%%%%
      % draw the embedding
      %\node(n1){n1};
      %\node(n2)[below right=3cm and 3cm of n1]{n2};
      % \node [text width = 4cm] at (3.5,8) {};
      %\draw[->] (P1)--(text2)--(n2) node[right] {};
      %\draw[->] (0,5)--(text)--(7,5) node[right] {};
      \draw[->,color=gray,dashed,very thick] (P1) to[out=45,in=135] (P2);

      \node [fill=blue,circle,minimum width=0.05cm] at (P2) {};
      \node [below,yshift=-0.15cm,xshift=0.6cm, text width=3cm] at (P2) {{\small RKHS embedding of $\mathbb{P}(Y|X=x)$}};
      \node [fill=red,circle,minimum width=0.05cm] at (P1) {};
      \node [below,yshift=-0.15cm] at (P1) {{\small feature map of $x$}};

      \coordinate (n1) at (0,5);
      \coordinate (n2) at (7,5);
      %\node(n1){n1};
      %\node(n2)[below right=3cm and 3cm of n1]{n2};
      \path (n1) -- node[sloped, fill=yellow, rounded corners=.3cm, text width=3cm] (text) {{\small RKHS embedding of $\mathbb{P}(Y|X)$}} (n2);
      \draw[->, very thick] (n1)--(text)--(n2) node[right] {};
      %\draw[->] (0,5)--(text)--(7,5) node[right] {};

      %%%%%%%

      %\draw[->] (-0.2,9) -- (4,9) node[right] {$\by$};
      %\draw[->] (0,9) -- (0,11.2) node[above] {$p(\by|\bx)$};
      %\draw[color=red,smooth,domain=0:4,ultra thick] plot (\x,{9.2+1.8*exp(-(\x-2)*(\x-2)*0.5/0.3)});
      %\node [right] at (3,10) {$\mathbb{P}(Y|X=\x)$};
    \end{tikzpicture}
  %\caption{From marginal distribution to conditional distribution: unlike the embeddings shown in Figure \ref{fig:marginal-embedding}, the embedding of 
  %conditional distribution $\mathbb{P}(Y|X)$ is not a single element in the RKHS. Instead, it may be viewed as a family of Hilbert space embeddings of conditional distributions $\pp{P}(Y|X=\x)$ indexed by the conditioning variable $X$. In other words, the conditional mean embedding can be viewed as an operator from $\hbspace$ to $\hbspg$(cf. \S\ref{sec:regression-view}).}
  \label{fig:conditional-embedding-summary}
\end{figure}

\begin{itemize}
\item Embed conditional probability function as an operator
\vspace{1em}
\item Conditioning operator is a family of functions of $y$ indexed by $x$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conditional Mean Embedding}
\begin{itemize}
\item Going to need a series of definitions
\vspace{2em}
\item Goal is to write $\mathbb{E}_{Y \mid X}[g(Y)]$ as an inner product
    \vspace{1em}
    \begin{itemize}
    \item Recall that messages were rewritten as conditional expectations wrt
    other messages
    \vspace{1em}
    \item  We consider the case when $g$ is a message
    \end{itemize}
\vspace{1em}
\item Will get there by defining the conditional mean embedding
    then applying similar derivation as marginal mean embedding
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conditional Mean Embedding}
\begin{itemize}
\item Two Hilbert spaces $\mathscr{H},\mathscr{G}$ for RVs $X,Y$
\item Define uncentered cross-covariance operator
$$C_{YX} = \mathbb{E}_{YX}[\varphi_Y\otimes\phi_X],$$
for $f \in \mathscr{H}, g \in \mathscr{G}$ and $C_{YX}: \mathscr{H} \to \mathscr{G}$
\item Has property
$$\mathbb{E}_{YX}[g(Y)f(X)] = \langle g, C_{YX}f\rangle = \langle g\otimes f, C_{YX} \rangle,$$
which extends the evaluation property to two variables.
(Can have $Y = X$ for autocorrelation)

Pf:
\begin{align*}
\mathbb{E}_{YX}[g(Y)f(X)]
&= \mathbb{E}_{YX}[\langle g, \phi_Y\rangle \langle f, \phi_X \rangle]\\
&= \mathbb{E}_{YX}[\langle g, \langle f, \phi_X\rangle \phi_Y  \rangle]\\
&= \mathbb{E}_{YX}[\langle g, (\phi_X \otimes \phi_Y)f  \rangle]\\
&= \langle g, \mathbb{E}_{YX}[\phi_X \otimes \phi_Y]f  \rangle\\
&= \langle g, C_{YX}f  \rangle
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conditional Mean Embedding}
\begin{itemize}
\item Consider the property $C_{XX}\mathbb{E}_{Y\mid X}[g(Y)\mid X] = C_{XY}g$ (Fukumitsu 2004).

Pf: $\forall f \in \mathscr{H}$,
\begin{align*}
\langle f, C_{XX}\mathbb{E}_{Y\mid X}[g(Y)\mid X] \rangle
&= \mathbb{E}_{X}[f(X) \mathbb{E}_{Y \mid X}[g(Y) \mid X]]\\
&= \mathbb{E}_{XY}[f(X) g(Y)]\\
&= \langle f, C_{XY}g\rangle
\end{align*}
\item Using the above property,
\begin{align*}
\mathbb{E}_{Y\mid X=x}[g(Y)\mid X=x]
&= \langle \mathbb{E}_{Y\mid X}[g(Y)\mid X], \phi_x\rangle\\
&= \langle C_{XX}^{-1}C_{XY}g, \phi_x\rangle\\
&= \langle g, C_{YX}C_{XX}^{-1}\phi_x\rangle\\
\end{align*}
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conditional Mean Embedding: Empirical Estimation}
\begin{itemize}
\item Samples $\set{X^n}_{n=1}^N\sim \mathbb{P}(X), \set{Y^n}_{n=1}^N\sim \mathbb{P}(Y)$,
\item Feature matrices (assuming finite feature dim)
$$\Phi= [\phi(X^1), \cdots, \phi(X^N)] \in \R^{D\times N},$$
$$\Upsilon= [\varphi(Y^1), \cdots, \varphi(Y^N)] \in \R^{D\times N},$$
with feature maps $\phi,\varphi$ for Hilbert spaces $\mathscr{H},\mathscr{G}$
of functions from $\mcX\mapsto\R,\mcY\mapsto\R$ respectively
\item We can approximate cross-covariance operator with
$C_{YX} \approx \Upsilon\Phi^\top / m$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Product Space Embeddings}
\begin{itemize}
\item We have to deal with multiple neighbours and messages
\vspace{1em}
\item Consider product Hilbert space $\mathscr{H}^\otimes = \prod_i \mathscr{H}_i$
    and product feature map $\xi$, so that $\xi(x) = \otimes_i \phi_i(x)$,
    the feature maps of the underlying spaces
\vspace{1em}
\item Recall tensor product $(f \otimes g)h = \langle g, h \rangle f$ where $f,g,h\in\mathscr{H}$
\vspace{1em}
\item For the case of message passing,
    generalizes to $\prod_i \langle f, \phi(x) \rangle = \langle \otimes_u f, \xi(x) \rangle$ 
\vspace{1em}
\item Not totally clear on the details, but can view $\xi(x)$ as just another function
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Kernel Belief Propagation Messages}
\begin{itemize}
\item Back to message passing:
\begin{align*}
m_{ts}(x_s)
&= \mathbb{E}_{Xt \mid x_s}[\prod_{u\in\delta(t)\setminus\set{s}}m_{ut}(X_t)]\\
&= \mathbb{E}_{Xt \mid x_s}[\prod_{u\in\delta(t)\setminus\set{s}} \langle m_{ut}, \phi_{X_t}\rangle]\\
&= \mathbb{E}_{Xt \mid x_s}[\langle \otimes_{u\setminus s} m_{ut}, \xi_{X_t}\rangle]\\
&= \langle \otimes_{u\setminus s} m_{ut}, \mathbb{E}_{Xt \mid x_s}[\xi_{X_t}]\rangle\\
&= \langle \otimes_{u\setminus s} m_{ut}, C_{X_t^\otimes X_s}C_{X_sX_s}^{-1}\phi_{x_s}\rangle
\end{align*}
\item And that's it! Now for empirical estimation and efficient computation
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Empirical Estimation}
\begin{itemize}
\item Key point: Avoids ever instantiating tensor products.\footnote{This is what messed up the runtime
    in my version of kernelized inference. Not sure if this transfers to our setting,
    but likely does since it seems to be a property of tensor products.}
\item $N$ data points, $D$ feature dim
\item Feature matrices
$\Phi, \Upsilon, \Phi^\otimes: R^{D \times N}$ for $X_t, X_s, X_t^\otimes$
and kernels $K = \Phi^\top\Phi, L = \Upsilon^\top\Upsilon$
\begin{align*}
m_{ts}(x_s)
&= \langle \otimes_{u\setminus s} m_{ut}, C_{X_t^\otimes X_s}C_{X_sX_s}^{-1}\phi_{x_s}\rangle\\
&\approx \langle \otimes_{u\setminus s} \Phi\beta_{ut},
    \Phi^\otimes L^{-1}\phi_{x_s}\Upsilon^\top\rangle\\
&= (\odot_{u\setminus s} K\beta_{ut})^\top L^{-1}\Upsilon^\top\phi_{x_s},
\end{align*}
where $\beta_{ut}\in\R^N$ is some function of $K,L$ and neighouring $\beta$.\footnote{
Description unclear in this paper, but covered in Song's Nonparametric Tree Graphical Models via 
Kernel Embeddings paper}
\item Upfront $O(N^3)$ matrix inversion cost, $O(|\delta^*|N^2)$ cost per message
\item Approximations use low-rank approximations of kernel matrices and tensor product
to limit dependence on $N$ and $|\delta^*|$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Conclusion}
\begin{itemize}
\item Approximations not applicable in Performer setting,
    would just subsample and reweight timesteps
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Definitions}
\begin{itemize}
\item Domain $\mcX$
\vspace{1em}
\item Hilbert space $\mathscr{H}$ of functions on $\mcX\mapsto\R$
    with inner product $\langle\cdot,\cdot\rangle$, kernel $K$,
    and feature map $\phi$
\vspace{1em}
\item The point evaluation property, 
ie that function evaluation is an inner product,
$$\langle f, K(x, \cdot) \rangle = f(x),$$
implies the reproducing property:
$$ \langle K(x,\cdot), K(y, \cdot) \rangle = K(x,y) = \langle \phi(x), \phi(y) \rangle$$
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Theorem Notes}
\begin{itemize}
\item Riesz representation theorem: If operator $\mcA: \mathscr{H} \to \R$
is bounded, then there exists a representer $g_\mcA \in \mathscr{H}$ st
$$A[f] = \langle f, g_\mcA \rangle, \forall f \in \mathscr{H}.$$
\item Point evaluation property: In an RKHS, consider the evaluation functional
$\mcF_\bx(f) = f(\bx)$. Riesz representation theorem tells us there exists a
representer $k_\bx: \mathscr{H}\to\R$ st
$$\mcF_\bx(f) = \langle f, k_\bx\rangle = f(\bx),$$
referred to as the reproducing kernel for the point $\bx$.
\item The reproducing property is a special case of the point evaluation
property. Consider the kernel $k: \mcX \times \mcX \to \R$,
and define $f(\bx) = k(\by, \bx)$ for all $\by \in \mcX$.
Applying the point evaluation property yields
$$f(\bx) = \langle k(\bx, \cdot),  k(\by, \cdot)\rangle,$$
where $k(\bx, \cdot)$ is the canonical feature map denoted by $\phi: \mcX\to\mathscr{H}$.
\item Alternatively you can start by assuming the kernel is positive definite ()
\end{itemize}
\end{frame}

\begin{frame}
\frametitle{Refs}
\bibliography{references}
\end{frame}

\end{document}
